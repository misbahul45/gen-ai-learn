{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfa48350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv('../data/IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d1b2c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   review     50000 non-null  object\n",
      " 1   sentiment  50000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 781.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e7b6439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ee6edad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd74565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# firts processing by make all to become lowercase\n",
    "df['review']=df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "174cbba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing by remove html tag\n",
    "import re   #regex\n",
    "def remove_html_tag(text):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1808a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1 : Hello, world!\n",
      "Text 2 : This is bold and italic text.\n",
      "Text 3 : Visit \n",
      "Text 4 : alert('XSS');Welcome\n",
      "Text 5 : Test nested tags\n"
     ]
    }
   ],
   "source": [
    "text_cases = [\n",
    "    \"<h1>Hello, world!</h1>\",\n",
    "    \"<p>This is <b>bold</b> and <i>italic</i> text.</p>\",\n",
    "    '<a href=\"https://example.com\">Visit</a> <img src=\"image.jpg\" />',\n",
    "    \"<script>alert('XSS');</script>Welcome\",\n",
    "    \"<div><p>Test <span>nested</span> tags</p></div>\",\n",
    "]\n",
    "\n",
    "for i, text in enumerate(text_cases, 1):\n",
    "    print(f\"Text {i} : {remove_html_tag(text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb9164b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"basically there's a family where a little boy (jake) thinks there's a zombie in his closet & his parents are fighting all the time.this movie is slower than a soap opera... and suddenly, jake decides to become rambo and kill the zombie.ok, first of all when you're going to make a film you must decide if its a thriller or a drama! as a drama the movie is watchable. parents are divorcing & arguing like in real life. and then we have jake with his closet which totally ruins all the film! i expected to see a boogeyman similar movie, and instead i watched a drama with some meaningless thriller spots.3 out of 10 just for the well playing parents & descent dialogs. as for the shots with jake: just ignore them.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][3]\n",
    "df['review']=df['review'].apply(remove_html_tag)\n",
    "df['review'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb4f084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing by clearing a url\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abfc83a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: ❌ FAILED\n",
      "Expected: Visit our website at for more info.\n",
      "Got: Visit our website at  for more info.\n",
      "Test 2: ❌ FAILED\n",
      "Expected: Check out and let us know.\n",
      "Got: Check out  and let us know.\n",
      "Test 3: ✅ PASSED\n",
      "Test 4: ❌ FAILED\n",
      "Expected: Multiple links:\n",
      "Got: Multiple links:  \n",
      "Test 5: ❌ FAILED\n",
      "Expected: Start with URL: and end with\n",
      "Got: Start with URL:  and end with \n"
     ]
    }
   ],
   "source": [
    "test_cases = [\n",
    "    {\n",
    "        \"input\": \"Visit our website at https://example.com for more info.\",\n",
    "        \"expected\": \"Visit our website at for more info.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Check out www.google.com and let us know.\",\n",
    "        \"expected\": \"Check out and let us know.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"No URLs in this sentence.\",\n",
    "        \"expected\": \"No URLs in this sentence.\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Multiple links: https://a.com https://b.org\",\n",
    "        \"expected\": \"Multiple links:\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Start with URL: http://start.com and end with www.end.com\",\n",
    "        \"expected\": \"Start with URL: and end with\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    result = remove_urls(case[\"input\"])\n",
    "    status = \"✅ PASSED\" if result == case[\"expected\"] else f\"❌ FAILED\\nExpected: {case['expected']}\\nGot: {result}\"\n",
    "    print(f\"Test {i}: {status}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7063604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing by deleting a spesial char like !\"#$%&'()*+,-./:;<=>?@[\\]^_{|}~\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d63cf30e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. the plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). while some may be disappointed when they realize this is not match point 2: risk addiction, i thought it was proof that woody allen is still fully in control of the style many of us have grown to love.this was the most i\\'d laughed at one of woody\\'s comedies in years (dare i say a decade?). while i\\'ve never been impressed with scarlet johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.this may not be the crown jewel of his career, but it was wittier than \"devil wears prada\" and more interesting than \"superman\" a great comedy to go see with friends.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "233862d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i thought this was a wonderful way to spend time on a too hot summer weekend sitting in the air conditioned theater and watching a lighthearted comedy the plot is simplistic but the dialogue is witty and the characters are likable even the well bread suspected serial killer while some may be disappointed when they realize this is not match point 2 risk addiction i thought it was proof that woody allen is still fully in control of the style many of us have grown to lovethis was the most id laughed at one of woodys comedies in years dare i say a decade while ive never been impressed with scarlet johanson in this she managed to tone down her sexy image and jumped right into a average but spirited young womanthis may not be the crown jewel of his career but it was wittier than devil wears prada and more interesting than superman a great comedy to go see with friends'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text=remove_punctuation(df['review'][2])\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5805d9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33165cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FYI': 'For Your Information',\n",
       " 'ASAP': 'As Soon As Possible',\n",
       " 'BRB': 'Be Right Back',\n",
       " 'BTW': 'By The Way',\n",
       " 'OMG': 'Oh My God',\n",
       " 'IMO': 'In My Opinion',\n",
       " 'LOL': 'Laugh Out Loud',\n",
       " 'TTYL': 'Talk To You Later',\n",
       " 'GTG': 'Got To Go',\n",
       " 'TTYT': 'Talk To You Tomorrow',\n",
       " 'IDK': \"I Don't Know\",\n",
       " 'TMI': 'Too Much Information',\n",
       " 'IMHO': 'In My Humble Opinion',\n",
       " 'ICYMI': 'In Case You Missed It',\n",
       " 'AFAIK': 'As Far As I Know',\n",
       " 'FAQ': 'Frequently Asked Questions',\n",
       " 'TGIF': \"Thank God It's Friday\",\n",
       " 'FYA': 'For Your Action'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# processing chat word\n",
    "chat_words = {\n",
    "    'AFAIK':'As Far As I Know',\n",
    "    'AFK':'Away From Keyboard',\n",
    "    'ASAP':'As Soon As Possible'\n",
    "}\n",
    "\n",
    "\n",
    "{\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"OMG\": \"Oh My God\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"LOL\": \"Laugh Out Loud\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"GTG\": \"Got To Go\",\n",
    "    \"TTYT\": \"Talk To You Tomorrow\",\n",
    "    \"IDK\": \"I Don't Know\",\n",
    "    \"TMI\": \"Too Much Information\",\n",
    "    \"IMHO\": \"In My Humble Opinion\",\n",
    "    \"ICYMI\": \"In Case You Missed It\",\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"TGIF\": \"Thank God It's Friday\",\n",
    "    \"FYA\": \"For Your Action\",\n",
    "    \"ICYMI\": \"In Case You Missed It\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7915b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing text to normalization text\n",
    "def chat_confersion(text):\n",
    "    new_text=[]\n",
    "\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words:\n",
    "            new_text.append(chat_words[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2b00a5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do This work As Soon As Possible'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_confersion('Do This work ASAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec47fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def incorrect_text(text):\n",
    "    blob = TextBlob(text)\n",
    "    corrected = blob.correct()\n",
    "    return str(corrected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c204afd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have good spelling\n"
     ]
    }
   ],
   "source": [
    "print(incorrect_text(\"I havv goood speling\"))\n",
    "# Output: \"I have good spelling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec45f4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/taqin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text, lang='english'):\n",
    "    stop_words = set(stopwords.words(lang))\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75a9c864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop word \n",
    "df['review']=df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ae0d701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing by removng emoji\n",
    "import re\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        \"[\"\n",
    "        \"\\U0001F600-\\U0001F64F\"  # emotikon wajah\n",
    "        \"\\U0001F300-\\U0001F5FF\"  # simbol & ikon\n",
    "        \"\\U0001F680-\\U0001F6FF\"  # transportasi & simbol lainnya\n",
    "        \"\\U0001F1E0-\\U0001F1FF\"  # bendera negara\n",
    "        \"\\U00002700-\\U000027BF\"  # simbol tambahan\n",
    "        \"\\U0001F900-\\U0001F9FF\"  # emoji tambahan\n",
    "        \"\\U00002600-\\U000026FF\"  # simbol cuaca dll\n",
    "        \"\\U00002B50-\\U00002B55\"\n",
    "        \"]+\",\n",
    "        flags=re.UNICODE\n",
    "    )\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d01450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Halo , ini adalah contoh teks  dengan emoji !\n"
     ]
    }
   ],
   "source": [
    "text = \"Halo 🌟, ini adalah contoh teks 😊 dengan emoji 🚀!\"\n",
    "cleaned_text = remove_emoji(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2521159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing by get emoji\n",
    "import emoji\n",
    "\n",
    "\n",
    "def get_emoji(text):\n",
    "    arr_emoji=[]\n",
    "    for char in text:\n",
    "        if emoji.is_emoji(char):\n",
    "            arr_emoji.append(char)\n",
    "    return arr_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6dde9363",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['🌟', '😊', '🚀']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Halo 🌟, ini adalah contoh teks 😊 dengan emoji 🚀!\"\n",
    "get_emoji(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd82a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Token dengan split() ===\n",
      "['Halo!', 'Ini', 'adalah', 'contoh', 'kalimat,', 'dengan', 'berbagai', 'tanda', 'baca.', 'Apakah', 'kamu', 'siap?', '💡🚀']\n",
      "\n",
      "=== Token dengan regex (re) ===\n",
      "['Halo', 'Ini', 'adalah', 'contoh', 'kalimat', 'dengan', 'berbagai', 'tanda', 'baca', 'Apakah', 'kamu', 'siap']\n",
      "\n",
      "=== Token dengan NLTK ===\n",
      "['Halo', '!', 'Ini', 'adalah', 'contoh', 'kalimat', ',', 'dengan', 'berbagai', 'tanda', 'baca', '.', 'Apakah', 'kamu', 'siap', '?', '💡🚀']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/taqin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# processing text by tokenization or splitting\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Contoh teks\n",
    "text = \"Halo! Ini adalah contoh kalimat, dengan berbagai tanda baca. Apakah kamu siap? 💡🚀\"\n",
    "\n",
    "# 1. Tokenisasi dengan split()\n",
    "tokens_split = text.split()\n",
    "\n",
    "# 2. Tokenisasi dengan re (mengambil hanya kata tanpa tanda baca)\n",
    "tokens_re = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# 3. Tokenisasi dengan nltk\n",
    "tokens_nltk = word_tokenize(text)\n",
    "\n",
    "# Output hasil\n",
    "print(\"=== Token dengan split() ===\")\n",
    "print(tokens_split)\n",
    "\n",
    "print(\"\\n=== Token dengan regex (re) ===\")\n",
    "print(tokens_re)\n",
    "\n",
    "print(\"\\n=== Token dengan NLTK ===\")\n",
    "print(tokens_nltk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148bac17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fli', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "# processing by change text to become baseline text\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Inisialisasi stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Contoh penggunaan\n",
    "words = [\"running\", \"flies\", \"easily\", \"fairly\"]\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5aed3184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/taqin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/taqin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/taqin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# inisiasi\n",
    "word_lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def clean_and_lemmatize(text):\n",
    "    # tokenisasi\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # remove spesial char and change it to lowercase\n",
    "    tokens = [word.lower() for word in tokens if word not in string.punctuation]\n",
    "\n",
    "    # lemmatize token\n",
    "    lemmatize_tokens = [word_lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    return \" \".join(lemmatize_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d7b3b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat were running quickly but they were n't faster than the dog\n"
     ]
    }
   ],
   "source": [
    "text = \"The cats were running quickly, but they weren't faster than the dog.\"\n",
    "\n",
    "cleaned = clean_and_lemmatize(text)\n",
    "\n",
    "print(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ae5d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/taqin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/taqin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/taqin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/taqin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# lemmatazion with improvemnet using post tagging for detect adjective word\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def clean_and_lemmatize_v2(text):\n",
    "    tokens=word_tokenize(text)\n",
    "\n",
    "    # remove punctuation\n",
    "    tokens=[word.lower() for word in tokens if word not in string.punctuation]\n",
    "\n",
    "    # pos tagging\n",
    "    post_tags=nltk.pos_tag(tokens)\n",
    "\n",
    "    # lemmatize with post tagging\n",
    "\n",
    "    lemma_text=[\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
    "        for word, pos in post_tags\n",
    "    ]\n",
    "\n",
    "    return \" \".join(lemma_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df6c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The cats were running quickly, but they weren't faster than the dog that ate the food.\"\n",
    "\n",
    "print(clean_and_lemmatize_v2(text))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
