{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b159678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìå Vocab: ['belajar', 'learning', 'machine', 'menarik', 'sangat', 'saya', 'suka']\n",
      "\n",
      "üî¢ Bag of Words (BoW):\n",
      "[1, 1, 1, 0, 0, 1, 1]\n",
      "[0, 1, 1, 1, 1, 0, 0]\n",
      "[1, 0, 0, 0, 0, 1, 1]\n",
      "\n",
      "üî¢ Binary BoW (Document Level):\n",
      "[1, 1, 1, 0, 0, 1, 1]\n",
      "[0, 1, 1, 1, 1, 0, 0]\n",
      "[1, 0, 0, 0, 0, 1, 1]\n",
      "\n",
      "üî† Word-Level One-Hot (Dokumen 1):\n",
      "saya ‚Üí [0, 0, 0, 0, 0, 1, 0]\n",
      "suka ‚Üí [0, 0, 0, 0, 0, 0, 1]\n",
      "belajar ‚Üí [1, 0, 0, 0, 0, 0, 0]\n",
      "machine ‚Üí [0, 0, 1, 0, 0, 0, 0]\n",
      "learning ‚Üí [0, 1, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# üì¶ Manual Text Encoding Utils\n",
    "# ============================\n",
    "\n",
    "def tokenize_documents(raw_documents):\n",
    "    return [doc.lower().split() for doc in raw_documents]\n",
    "\n",
    "def build_vocab(tokenized_documents):\n",
    "    return sorted(set(word for doc in tokenized_documents for word in doc))\n",
    "\n",
    "# 1Ô∏è‚É£ Manual Bag of Words (BoW)\n",
    "def manual_bow(tokenized_documents, vocab):\n",
    "    vectors = []\n",
    "    for doc in tokenized_documents:\n",
    "        vector = [doc.count(word) for word in vocab]\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "# 2Ô∏è‚É£ Manual One-Hot Encoding (Word Level)\n",
    "def word_to_one_hot(word, vocab_dict):\n",
    "    vec = [0] * len(vocab_dict)\n",
    "    if word in vocab_dict:\n",
    "        vec[vocab_dict[word]] = 1\n",
    "    return vec\n",
    "\n",
    "def document_word_level_one_hot(tokenized_document, vocab_dict):\n",
    "    return [word_to_one_hot(word, vocab_dict) for word in tokenized_document]\n",
    "\n",
    "# 3Ô∏è‚É£ Manual One-Hot (Binary BoW / Doc Level)\n",
    "def manual_binary_bow(tokenized_documents, vocab):\n",
    "    vectors = []\n",
    "    for doc in tokenized_documents:\n",
    "        vector = [1 if word in doc else 0 for word in vocab]\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "# 4Ô∏è‚É£ Gabungan Word-Level + Dokumen\n",
    "def combined_word_doc_one_hot(tokenized_documents, vocab_dict):\n",
    "    result = []\n",
    "    for doc in tokenized_documents:\n",
    "        one_hot_words = []\n",
    "        for word in doc:\n",
    "            one_hot_words.append((word, word_to_one_hot(word, vocab_dict)))\n",
    "        result.append(one_hot_words)\n",
    "    return result\n",
    "\n",
    "# ============================\n",
    "# üß™ Contoh Penggunaan\n",
    "# ============================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    documents = [\n",
    "        \"saya suka belajar machine learning\",\n",
    "        \"machine learning sangat menarik\",\n",
    "        \"saya suka belajar\"\n",
    "    ]\n",
    "\n",
    "    tokenized_docs = tokenize_documents(documents)\n",
    "    vocab = build_vocab(tokenized_docs)\n",
    "    vocab_dict = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "    print(\"üìå Vocab:\", vocab)\n",
    "\n",
    "    print(\"\\nüî¢ Bag of Words (BoW):\")\n",
    "    for vec in manual_bow(tokenized_docs, vocab):\n",
    "        print(vec)\n",
    "\n",
    "    print(\"\\nüî¢ Binary BoW (Document Level):\")\n",
    "    for vec in manual_binary_bow(tokenized_docs, vocab):\n",
    "        print(vec)\n",
    "\n",
    "    print(\"\\nüî† Word-Level One-Hot (Dokumen 1):\")\n",
    "    for word, vec in combined_word_doc_one_hot(tokenized_docs, vocab_dict)[0]:\n",
    "        print(f\"{word} ‚Üí {vec}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf8b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['belajar' 'learning' 'machine' 'menarik' 'sangat' 'saya' 'suka']\n",
      "BoW Matrix:\n",
      " [[1 1 1 0 0 1 1]\n",
      " [0 1 1 1 1 0 0]\n",
      " [1 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "docs = [\n",
    "    \"saya suka belajar machine learning\",\n",
    "    \"machine learning sangat menarik\",\n",
    "    \"saya suka belajar\"\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"BoW Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d88e7aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['belajar' 'learning' 'machine' 'menarik' 'sangat' 'saya' 'suka']\n",
      "One-Hot (Binary) Matrix:\n",
      " [[1 1 1 0 0 1 1]\n",
      " [0 1 1 1 1 0 0]\n",
      " [1 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# one hot doc\n",
    "vectorizer_bin=CountVectorizer(binary=True)\n",
    "\n",
    "X_bin=vectorizer_bin.fit_transform(docs)\n",
    "\n",
    "\n",
    "print(\"Vocabulary:\", vectorizer_bin.get_feature_names_out())\n",
    "print(\"One-Hot (Binary) Matrix:\\n\", X_bin.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "580ac4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['belajar' 'learning' 'machine' 'menarik' 'sangat' 'saya' 'suka']\n",
      "TF-IDF Matrix:\n",
      " [[0.4472136  0.4472136  0.4472136  0.         0.         0.4472136\n",
      "  0.4472136 ]\n",
      " [0.         0.42804604 0.42804604 0.5628291  0.5628291  0.\n",
      "  0.        ]\n",
      " [0.57735027 0.         0.         0.         0.         0.57735027\n",
      "  0.57735027]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(docs)\n",
    "\n",
    "print(\"Vocabulary:\", tfidf.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", X_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa86e5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram: ['belajar nlp' 'saya suka' 'suka belajar']\n",
      "[[1 1 1]]\n",
      "[('saya', 'suka'), ('suka', 'belajar'), ('belajar', 'NLP')]\n"
     ]
    }
   ],
   "source": [
    "# n gram text representation for handling message\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\"saya suka belajar NLP\"]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2))  # bigram\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print(\"N-gram:\", vectorizer.get_feature_names_out())\n",
    "print(X.toarray())\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"saya suka belajar NLP\"\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "bigrams = list(ngrams(tokens, 2))\n",
    "print(bigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca5ebe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Membaca PDF dari '../text/text-1.pdf'...\n",
      "üßπ Preprocessing teks...\n",
      "üß† Training Word2Vec...\n",
      "üî¢ Menghitung vektor dokumen...\n",
      "\n",
      "‚úÖ Proses Selesai!\n",
      "   Shape vektor dokumen: (100,)\n",
      "   Vektor dokumen (10 elemen pertama): [-9.86640691e-04  1.57771318e-03  6.12215896e-04  6.41604362e-04\n",
      "  4.66454978e-04 -2.80584698e-03  1.25432049e-03  4.56800032e-03\n",
      " -2.55997456e-03 -1.33377558e-03 -1.07330538e-03 -3.26242903e-03\n",
      " -2.95098725e-04  1.30282994e-03  1.17111404e-03 -1.22856896e-03\n",
      "  1.27894466e-03 -1.38315524e-03 -1.38783734e-03 -4.60860692e-03\n",
      "  9.04934772e-04  5.94044977e-04  1.87951420e-03 -9.64887266e-04\n",
      " -1.27456238e-04  2.48710858e-04 -1.75537600e-03 -6.22847700e-04\n",
      " -1.40363164e-03  6.93301437e-04  1.95686566e-03 -2.54198705e-04\n",
      "  6.96307514e-04 -1.93546771e-03 -1.65105704e-03  2.27620057e-03\n",
      "  8.87568807e-04 -1.94619247e-03 -1.12640450e-03 -2.92021688e-03\n",
      "  2.39968547e-04 -1.77604274e-03 -9.66460328e-04 -1.04185296e-04\n",
      "  1.78332918e-03 -4.98769223e-04 -1.39121234e-03 -2.83539935e-04\n",
      "  1.29950768e-03  1.29289192e-03  1.52074534e-03 -1.99133414e-03\n",
      " -8.23174603e-04 -4.99611953e-04 -1.17083895e-03  3.51602706e-04\n",
      "  1.24436768e-03  2.62256617e-05 -2.76487111e-03  1.10098487e-03\n",
      "  2.20956965e-04  2.16657136e-04  5.59916545e-04 -5.59498498e-04\n",
      " -2.80055869e-03  2.59698485e-03  1.49309181e-03  2.12210068e-03\n",
      " -2.83741509e-03  2.44262186e-03 -7.73990410e-04  1.13074272e-03\n",
      "  2.23659747e-03 -3.08772025e-04  1.67195534e-03  1.47562160e-03\n",
      "  3.29965696e-04  2.01160787e-04 -1.69933587e-03  1.02705815e-06\n",
      " -1.54333003e-03 -9.57246230e-04 -2.19179201e-03  2.89306580e-03\n",
      " -5.53191814e-04 -1.33523965e-04  5.85112371e-04  2.21363548e-03\n",
      "  2.56185234e-03  3.34572978e-04  2.67420034e-03  1.09180226e-03\n",
      "  4.83842596e-04 -3.89437155e-05  3.31868231e-03  2.04012939e-03\n",
      "  1.28151395e-03 -2.21757498e-03  9.10307164e-04 -4.23411722e-04]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üìÑ PDF ke Word2Vec Embedding - Gensim\n",
    "# ==========================================\n",
    "\n",
    "# ‚úÖ Install package (jalankan di terminal sebelum pakai):\n",
    "# pip install pymupdf gensim nltk\n",
    "\n",
    "import fitz  # PyMuPDF untuk membaca isi PDF\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import nltk\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# üì• Unduh resource NLTK yang dibutuhkan (tokenizer dan stopwords)\n",
    "# Cukup jalankan sekali saja, setelah itu bisa diberi komentar\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 1Ô∏è‚É£ Fungsi: Baca PDF dan ubah ke string\n",
    "# ------------------------------------------\n",
    "def pdf_to_text(path):\n",
    "    \"\"\"\n",
    "    Membaca file PDF dan menggabungkan semua teks dari setiap halaman menjadi satu string.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 2Ô∏è‚É£ Fungsi: Preprocessing Teks\n",
    "# ------------------------------------------\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Lowercase, tokenisasi, hapus stopwords dan tanda baca.\n",
    "    \"\"\"\n",
    "    # Menggunakan stopwords bahasa Inggris sesuai dengan default NLTK\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Tokenisasi\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Hapus stopwords dan tanda baca\n",
    "    tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3Ô∏è‚É£ Fungsi: Split ke kalimat dan tokenize\n",
    "# ------------------------------------------\n",
    "def get_sentences(text):\n",
    "    \"\"\"\n",
    "    Memecah teks menjadi kalimat, lalu memproses tiap kalimat menjadi token.\n",
    "    Dibutuhkan untuk input Word2Vec (list of list).\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    # Memastikan tidak ada list kosong yang masuk ke model Word2Vec\n",
    "    return [tokens for tokens in (preprocess(sentence) for sentence in sentences) if tokens]\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4Ô∏è‚É£ Fungsi: Konversi dokumen ke vektor\n",
    "# ------------------------------------------\n",
    "def document_vector(model, doc_tokens):\n",
    "    \"\"\"\n",
    "    Mendapatkan representasi vektor dari dokumen dengan menghitung\n",
    "    rata-rata dari semua vektor kata yang ada di dalam model Word2Vec.\n",
    "    \"\"\"\n",
    "    # Filter token yang ada di vocabulary model saja\n",
    "    tokens = [token for token in doc_tokens if token in model.wv]\n",
    "    \n",
    "    # Jika tidak ada token dari dokumen yang ada di model, kembalikan vektor nol\n",
    "    if not tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "        \n",
    "    return np.mean(model.wv[tokens], axis=0)\n",
    "\n",
    "\n",
    "# ==========================================\n",
    "# üöÄ EKSEKUSI\n",
    "# ==========================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # üîπ Masukkan nama file PDF\n",
    "    # Pastikan path ini benar sesuai lokasi file Anda\n",
    "    file_path = \"../text/text-1.pdf\"  # ganti dengan path PDF kamu\n",
    "\n",
    "    try:\n",
    "        # 1. Ekstraksi teks dari PDF\n",
    "        print(f\"üìÑ Membaca PDF dari '{file_path}'...\")\n",
    "        pdf_text = pdf_to_text(file_path)\n",
    "\n",
    "        # 2. Tokenisasi kalimat dan kata\n",
    "        print(\"üßπ Preprocessing teks...\")\n",
    "        sentences = get_sentences(pdf_text)\n",
    "        \n",
    "        if not sentences:\n",
    "            print(\"‚ùå Tidak ada teks yang bisa diproses setelah preprocessing. Cek isi PDF Anda.\")\n",
    "        else:\n",
    "            # 3. Latih model Word2Vec\n",
    "            print(\"üß† Training Word2Vec...\")\n",
    "            # min_count=1 agar model tetap terlatih meskipun dokumen pendek\n",
    "            model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "            # 4. Ambil vektor dokumen\n",
    "            print(\"üî¢ Menghitung vektor dokumen...\")\n",
    "            full_tokens = preprocess(pdf_text)\n",
    "            doc_vector = document_vector(model, full_tokens)\n",
    "\n",
    "            print(\"\\n‚úÖ Proses Selesai!\")\n",
    "            print(\"   Shape vektor dokumen:\", doc_vector.shape)\n",
    "            print(\"   Vektor dokumen (10 elemen pertama):\", doc_vector)\n",
    "\n",
    "            # 5. Simpan model (opsional)\n",
    "            # model.save(\"word2vec_pdf.model\")\n",
    "            # print(\"üíæ Model disimpan sebagai word2vec_pdf.model\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: File tidak ditemukan di path '{file_path}'. Pastikan nama dan lokasi file sudah benar.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Terjadi error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
